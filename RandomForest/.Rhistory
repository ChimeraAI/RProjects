View(dataFrame)
help(apply)
apply(DataFrame,2,function(x) length(unique(x)))
apply(dataFrame,2,function(x) length(unique(x)))
help(c
)
for(i in cols) {
dataFrame[,i] = as.factor(dataFrame[,i])
}
cols <- c("job","marital","education","default","housing","loan","contact","day","poutcome","y")
barplot(table(dataFrame$y))
barplot(table(dataFrame$day))
barplot(table(dataFrame$previous))
barplot(table(dataFrame$pdays))
barplot(table(dataFrame$pdays))
table(dataFrame$pdays)
barplot(able(dataFrame$campaign))
barplot(table(dataFrame$campaign))
barplot(table(dataFrame$duration))
barplot(table(dataFrame$day))
barplot(table(dataFrame$day))
source('~/PersonalProjects/RProject/RandomForest/Code.R')
barplot(table(dataFrame$contact))
barplot(table(dataFrame$contact))
source('~/PersonalProjects/RProject/RandomForest/Code.R')
barplot(table(dataFrame$contact))
apply(dataFrame,2,function(x) length(unique(x)))
barplot(table(dataFrame$month))
barplot(table(dataFrame$contact))
barplot(table(dataFrame$loan))
barplot(table(dataFrame$balance))
barplot(table(dataFrame$education))
help(~)
help("~")
getwd()
read.csv("bank-full.csv")
View(dataFrame)
View(dataFrame)
dataFrame %>% summary()
data("iris")
factor(dataFrame$poutcome)
unique(factor(dataFrame$poutcome))
import(magrittr)
library(magrittr)
install.packages("magrittr")
barplot(table(dataFrame$poutcome))
barplot(table(dataFrame$balance))
barplot(table(dataFrame$poutcome))
table(dataFrame$poutcome)(2)
help(apply)
testing <- data.frame(dataFrame$poutcome,dataFrame$y)
View(testing)
View(testing)
dataFrame[1,1]
testing[1,1]
help("read.csv")
rm(testing)
for(i in testing) {
print(i)
}
for(i in testing) {
print(i)
}
source('~/GitHub/RProjects/RandomForest/Code.R')
print(i[0])
source('~/GitHub/RProjects/RandomForest/Code.R')
dim(testing)
dim(testin[:,])
dim(testin[:][0])
dim(testing[:,])
dim(testing)(0)
dim(testing)[0]
dd <- dim(testing)
dd[0]
dd[1]
for(i in testing) {
print(i[2])
}
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
nrow(testing)
source('~/GitHub/RProjects/RandomForest/Code.R')
identical(1,1)
summary(dataFrame$contact)
help(rpart)
plot(mpg ~ cyl, data = mtcars)
help("plot")
plot(sin,-pi,2*pi)
mtcars
plot(mpg ~ cyl, data = mtcars)
plot(~.,data=mtcars)
help(~.)
?formula
plot(~.,data = iris)
plot(mpg ~., data = mtcars)
set.seed(22)
names(getModelInfo())
install.packages(caret)
"caret"
install.packages("caret")
names(getModelInfo())
library(caret)
names(getModelInfo())
help(trainControl)
n = c(2,3,5)
s = c("aa","bb","CC")
b = c(TRUE,FALSE,TRUE)
df = data.frame(n,s,b)
df
help(names())
help(names)
source('~/GitHub/RProjects/RandomForest/TestCode.R')
install.packages("RCurl")
source('~/GitHub/RProjects/RandomForest/TestCode.R')
install.packages(bitops)
install.packages("bitops")
install.packages("bitops")
source('~/GitHub/RProjects/RandomForest/TestCode.R')
source('~/GitHub/RProjects/RandomForest/TestCode.R')
View(vehicles)
vehicles <- vehicles[sample(nrow(vehicles)),]
source('~/GitHub/RProjects/RandomForest/TestCode.R')
View(vehicles)
rm(list = ls())
vehicles$cylinders <- ifelse(vehicles$cylinders == 6, 1,0)
source('~/GitHub/RProjects/RandomForest/TestCode.R')
source('~/GitHub/RProjects/RandomForest/TestCode.R')
ensembleData[0:20,]
ensembleData[0:2,]
ensembleData[0:2]
sample(nrow(vechicles))
sample(nrow(vehicles))
rm( list = ls())
source('~/GitHub/RProjects/RandomForest/TestCode.R')
View(vehicles)
source('~/GitHub/RProjects/RandomForest/TestCode.R')
sample(nrow(vehicles))
nrow(vehicles)
help("sample")
size(sample(nrow(vehicles)))
length(sample(nrow(vehicles)))
vehicles[(1,2,3),]
vehicles[1,]
vehicles[[1,2,3],]
vehicles[c(1,2,3),]
vehicles[c(30,2),]
help(traincontrol)
help("trainControl")
source('~/GitHub/RProjects/RandomForest/TestCode.R')
help("trainControl")
help("train")
predictors
names(ensembleData)
model_treebag <- train(ensembleData[,predictors], ensembleData[,labelName], method='treebag', trControl=myControl)
source('~/GitHub/RProjects/RandomForest/TestCode.R')
warnings()
help(predict)
blenderData$gbm_PROB <- predict(object=model_gbm, blenderData[,predictors])
View(blenderData)
source('~/GitHub/RProjects/RandomForest/TestCode.R')
rm( list = ls())
prop.table(table(vehicles$cylinders))
source('~/GitHub/RProjects/RandomForest/TestCode.R')
install.packages(pROC)
install.packages("pROC")
prop.table(table(vehicles$cylinders))
rm( list = ls())
library(caret)
names(getModelInfo())
# Load data from Hadley Wickham on Github - Vehicle data set and predict 6 cylinder vehicles
library(RCurl)
#urlData <- getURL('https://raw.githubusercontent.com/hadley/fueleconomy/master/data-raw/vehicles.csv')
#vehicles <- read.csv(text = urlData)
# alternative way of getting the data
urlfile <-'https://raw.githubusercontent.com/hadley/fueleconomy/master/data-raw/vehicles.csv'
x <- getURL(urlfile, ssl.verifypeer = FALSE)
vehicles <- read.csv(textConnection(x))
# clean up the data and only use the first 24 columns
vehicles <- vehicles[names(vehicles)[1:24]]
vehicles <- data.frame(lapply(vehicles, as.character), stringsAsFactors=FALSE)
vehicles <- data.frame(lapply(vehicles, as.numeric))
vehicles[is.na(vehicles)] <- 0
vehicles$cylinders <- ifelse(vehicles$cylinders == 6, 1,0)
prop.table(table(vehicles$cylinders))
# shuffle and split the data into three parts
set.seed(1234)
vehicles <- vehicles[sample(nrow(vehicles)),]
split <- floor(nrow(vehicles)/3)
ensembleData <- vehicles[0:split,]
blenderData <- vehicles[(split+1):(split*2),]
testingData <- vehicles[(split*2+1):nrow(vehicles),]
# set label name and predictors
labelName <- 'cylinders'
predictors <- names(ensembleData)[names(ensembleData) != labelName]
library(caret)
# create a caret control object to control the number of cross-validations performed
myControl <- trainControl(method='cv', number=3, returnResamp='none')
# quick benchmark model
test_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm', trControl=myControl)
test_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm', trControl=myControl)
test_model
myControl <- trainControl(method='cv', number=4, returnResamp='none')
test_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm', trControl=myControl)
test_model
help(n.trees)
preds <- predict(object=test_model, testingData[,predictors])
preds
clear
clear()
help(predict)
help(roc)
library(pROC)
library(pROC)
help(roc)
auc <- roc(testingData[,labelName], preds)
auc
help(rpart)
help("train")
help(randomForest)
help(rf)
help(xgbTree)
??xgbTree
??xgbboost
??xgboost
help(train)
fit.gbm <- train(dataFrame[,predictors], dataFrame[,labelName], method="gbm", trControl=myControl)
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
install.packages(caTools)
install.packages("caTools")
source('~/GitHub/RProjects/RandomForest/Code.R')
source('~/GitHub/RProjects/RandomForest/Code.R')
dotplot(boosting_result)
source('~/GitHub/RProjects/RandomForest/Code.R')
# Training data
fit.gbm <- train(dataFrame[,predictors], dataFrame[,labelName], method="gbm", trControl=myControl)
boosting_result <- resamples(gbm = fit.gbm)
summary(boosting_result)
dotplot(boosting_result)
boosting_result <- resamples(list(gbm = fit.gbm))
summary(fit.gbm)
dotplot(fit.gbm)
t <- table(predictions = predictionTest, actual = testData$y)
predictionTest <- predict(fit.gbm, testData, type = "class")
t <- table(predictions = predictionTest, actual = testData$y)
predictionTest <- predict(fit.gbm, testData, type = "prob")
t <- table(predictions = predictionTest, actual = testData$y)
View(predictionTest)
t <- table(predictions = predictionTest, actual = testData$y)
t <- table(predictions = unlist(predictionTest), actual = unlist(testData$y))
View(predictionTest)
length(predictionTest)
length(testData$y)
t <- table(predictions = t(unlist(predictionTest), actual = unlist(testData$y))
t <- table(predictions = t(unlist(predictionTest)), actual = unlist(testData$y))
t <- table(predictions = temp, actual = unlist(testData$y))
temp <- t(unlist(predictionTest))
t <- table(predictions = temp, actual = unlist(testData$y))
length(unlist(testData$y))
length(temp)
auc <- roc(testData[,labelName], predictionTest)
library(pROC)
print(auc$auc) # Area under the curve: 0.9896
install.packages("xgboost")
help(sparse.model.matrix)
rm(list = ls())
sparse_matrix <- sparse.model.matrix(data = dataFrame)
sparse_matrix <- sparse.model.matrix(data = dataFrame)
library(Matrix)
sparse_matrix <- sparse.model.matrix(data = dataFrame)
sparse_matrix <- sparse.model.matrix(y ~ .-1,data = dataFrame)
library(caret)
library(caTools)
library(xgboost)
library(Matrix)
dataFrame <- read.csv("bank-full.csv", sep = ";", header = TRUE)
apply(dataFrame,2,function(x) length(unique(x)))
sparse_matrix <- sparse.model.matrix(y ~ .-1,data = dataFrame)
sparse_matrix
View(dataFrame)
dataFrame[dataFrame$age < 14 | dataFrame$age > 100,'age'] <- -1
View(dataFrame)
View(dataFrame)
apply(dataFrame,2,function(x) length(unique(x)))
help("dummyVars")
library(caret)
library(caTools)
library(xgboost)
library(Matrix)
library(car)
dataFrame <- read.csv("bank-full.csv", sep = ";", header = TRUE)
apply(dataFrame,2,function(x) length(unique(x)))
sparse_matrix <- sparse.model.matrix(y ~ .-1,data = dataFrame)
output_vector = dataFrame[,y] == "yes"
ohe_feats = c("job","marital","education","default","housing","loan","contact","poutcome")
dummies <- dummyVars(~ job + marital + education + defualt + housing + loan + contact + poutcome, data = dataFrame)
df_all_ohe <- as.data.frame(predict(dummies, newdata = dataFrame))
dummies <- dummyVars(~ job + marital + education + defualt + housing + loan + contact + poutcome, data = dataFrame)
help("predict")
help("as.data.frame")
df_all_ohe <- as.data.frame(dummies, newdata = dataFrame)
dummies <- dummyVars(~ job + marital + education + defualt + housing + loan + contact + poutcome, data = dataFrame)
source('~/GitHub/RProjects/RandomForest/Code.R')
dummies <- dummyVars(~ job + marital + education + default + housing + loan + contact + poutcome, data = dataFrame)
# Implementation of xgboost
# Load libraries ---------------------------------------
library(caret)
library(caTools)
library(xgboost)
library(Matrix)
library(car)
dataFrame <- read.csv("bank-full.csv", sep = ";", header = TRUE)
apply(dataFrame,2,function(x) length(unique(x)))
sparse_matrix <- sparse.model.matrix(y ~ .-1,data = dataFrame)
output_vector = dataFrame[,y] == "yes"
ohe_feats = c("job","marital","education","default","housing","loan","contact","poutcome")
dummies <- dummyVars(~ job + marital + education + default + housing + loan + contact + poutcome, data = dataFrame)
df_all_ohe <- as.data.frame(dummies, newdata = dataFrame)
df_all_ohe <- as.data.frame(predict(dummies, newdata = dataFrame))
View(df_all_ohe)
View(df_all_ohe)
sparse_matrix
A = matrix()
A = matrix(
c(2,4,3,1,5,7)
nrow = 2,
ncol = 3,
byrow = TRUE)
A = matrix( c(2,4,3,1,5,7), nrow = 2, ncol = 3, byrow = TRUE)
A
A[,-1]
A
df_all_ohe <- data.frame(predict(dummies, newdata = dataFrame))
View(df_all_ohe)
View(df_all_ohe)
outcome <- dataFrame[, "y"]
outcome
length(levels(outcome))
outcome <- factor(dataFrame[, "y"])
outcome
help(local)
local({
levels(outcome) <- list("yes" = 1, "no" = 0)
numeric(outcome)
})
as.numeric(outcome)
outcome
featurePlot(df_all_ohe, outcome, "strip")
outcome <- factor(dataFrame[, "y"])
featurePlot(df_all_ohe, outcome, "strip")
y_pred <- predict(xgb, data.matrix(trainData[,!(names(trainData) %in% drop)]))
source('~/GitHub/RProjects/RandomForest/Code.R')
load(RCurl)
install.packages("RCurl")
trainData <- read.table(train.url, header = F, sep = ",", col.names = setcol, na.strings = c(" ?"), stringsAsFactors = F)
rm(list = ls())
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
trainData <- read.table(train.url, header = F, sep = ",", col.names = setcol, na.strings = c(" ?"), stringsAsFactors = F)
trainData <- read.csv(train.url)
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
trainData <- read.csv(testConnections(train.url))
trainData <- read.csv(testConnection(train.url))
trainData <- read.csv(textConnection(train.url))
View(trainData)
View(trainData)
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
testData <- read.table(textConnection(test.url))
View(trainData)
rm(list=ls())
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
View(trainData)
testData <- read.table(textConnection(test.url), header = F,sep = ",", col.names = setcol,skip = 1, na.strings = c(" ?"), stringsAsFactors = F)
help("textConnection")
data.frame(trainData)
data.frame(testData)
table(is.na(train))
table(is.na(trainData))
sapply(trainData, function(x) sum(is.na(x))/length(x))*100
table(is.na(test))
table(is.na(testData))
sapply(testData, function(x) sum(is.na(x))/length(x))*100
View(testData)
library(stringr)
testData[,target := substr(target, start = 1, stop = nchar(target) - 1)]
testData[,target <- substr(target, start = 1, stop = nchar(target) - 1)]
testData[,target <- substr(target, start = 1, stop = nchar(target) - 1)]
View(trainData)
View(testData)
setDT(trainData)
help(setDT)
setDT(trainData)
data.table.setDT(trainData)
install.packages("data.table")
install.packages("data.table")
source('~/GitHub/RProjects/RandomForest/TestCode2.R')
char_col <- colnames(train)[ sapply (test,is.character)]
char_col <- colnames(trainData)[ sapply (test,is.character)]
char_col <- colnames(trainData)[ sapply (test,is.character)]
char_col <- colnames(trainData)[ sapply (testData,is.character)]
char_col
help(sapply)
help("is.character")
for(i in char_col) set(trainData,j=i,value = str_trim(trainData[[i]],side = "left"))
for(i in char_col) set(testData,j=i,value = str_trim(testData[[i]],side = "left"))
testData[[1]]
testData[,1]
for(i in char_col) set(trainData,j=i,value = str_trim(trainData[,i],side = "left"))
for(i in char_col) set(trainData,j=i,value = str_trim(trainData[[i]],side = "left"))
for(i in char_col) set(testData,j=i,value = str_trim(testData[[i]],side = "left"))
testData[is.na(testData)] <- "Missing"
trainData[is.na(trainData)] <- "Missing"
trainData[is.na(trainData)] <- "Missing"
testData[is.na(testData)] <- "Missing"
labels <- trainData$target
ts_labels <- testData$target
new_tr <- model.matrix(~.+0,data = trainData[,-c("target"),with=F])
View(new_tr)
new_ts <- model.matrix(~.+0,data = test[,-c("target"),with=F])
new_ts <- model.matrix(~.+0,data = testData[,-c("target"),with=F])
View(new_ts)
labels <- as.numeric(labels)-1
labels
ts_label <- as.numeric(ts_label)-1
ts_label <- as.numeric(ts_labels)-1
help("model.matrix")
help(with)
labels <- as.numeric(as.character(labels))-1
labels
labels <- trainData$target
labels <- as.numeric(as.character(labels))-1
labels <- as.numeric(trainData$target)
labels <- as.numeric(as.factor(labels))-1
labels <- trainData$target
labels <- as.numeric(as.factor(labels))-1
labels
ts_labels <- testData$target
ts_label <- as.numeric(as.factor(ts_labels))-1
ts_labels <- as.numeric(as.factor(ts_labels))-1
ts_labels
dtrain <- xgb.DMatrix(data = new_tr,label = labels)
library(xgboost)
dtrain <- xgb.DMatrix(data = new_tr,label = labels)
dtrain
show(dtrain)
dtest <- xgb.DMatrix(data = new_ts,label=ts_labels)
dtest
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
min(xgbcv$test.error.mean)
min(xgbcv$test.error.mean)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
gbpred <- predict (xgb1,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred <- predict (xgb1,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
library(caret)
confusionMatrix (xgbpred, ts_label)
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
group <- factor( c(1,1,2,2) )
model.matrix(~ group + 0)
model.matrix(~ group)
model.matrix(~. + 0, data = group)
new_tr
install.packages(mlr)
install.packages('mlr')
library(mlr)
fact_col <- colnames(train)[sapply(train,is.character)]
fact_col <- colnames(trainData)[sapply(train,is.character)]
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
fact_col
for(i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
traintask <- makeClassifTask (data = train,target = "target")
traintask <- makeClassifTask (data = trainData,target = "target")
traintask
testtask <- makeClassifTask (data = testData,target = "target")
testtask
traintask <- createDummyFeatures (obj = traintask,target = "target")
testtask <- createDummyFeatures (obj = testtask,target = "target")
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
traintask
help(makeLearner)
testtask <- createDummyFeatures (obj = testtask)
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.1)
params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree","gblinear")), makeIntegerParam("max_depth",lower = 3L,upper = 10L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L), makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
ctrl <- makeTuneControlRandom(maxit = 10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
mytune$y
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
rm(list = ls())
dataFrame <- read.csv("bank-full.csv", sep = ";", header = TRUE)
View(dataFrame)
