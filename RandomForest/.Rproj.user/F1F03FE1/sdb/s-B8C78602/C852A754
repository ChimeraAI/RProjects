{
    "collab_server" : "",
    "contents" : "library(RCurl)\nlibrary(data.table)\nlibrary(xgboost)\n\ntrain.url <- getURL('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\ntest.url <- getURL('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test')\n\nsetcol <- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\",\n            \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"target\")\n\ntrainData <- read.table(textConnection(train.url), header = F, sep = \",\", col.names = setcol, na.strings = c(\" ?\"), stringsAsFactors = F)\ntestData <- read.table(textConnection(test.url), header = F,sep = \",\", col.names = setcol,skip = 1, na.strings = c(\" ?\"), stringsAsFactors = F)\n\nsetDT(trainData)\nsetDT(testData)\n\ntable(is.na(trainData))\nsapply(trainData, function(x) sum(is.na(x))/length(x))*100\n\ntable(is.na(testData))\nsapply(testData, function(x) sum(is.na(x))/length(x))*100\n\nlibrary(stringr)\ntestData[,target <- substr(target, start = 1, stop = nchar(target) - 1)]\n\nchar_col <- colnames(trainData)[ sapply (testData,is.character)]\nfor(i in char_col) set(trainData,j=i,value = str_trim(trainData[[i]],side = \"left\"))\n\nfor(i in char_col) set(testData,j=i,value = str_trim(testData[[i]],side = \"left\"))\n\ntrainData[is.na(trainData)] <- \"Missing\"\ntestData[is.na(testData)] <- \"Missing\"\n\nlabels <- trainData$target\nts_labels <- testData$target\nnew_tr <- model.matrix(~.+0,data = trainDataata[,-c(\"target\"),with=F])\nnew_ts <- model.matrix(~.+0,data = testData[,-c(\"target\"),with=F])\n\nlabels <- as.numeric(as.factor(labels))-1\nts_labels <- as.numeric(as.factor(ts_labels))-1\n\ndtrain <- xgb.DMatrix(data = new_tr,label = labels)\n\ndtest <- xgb.DMatrix(data = new_ts,label = ts_labels)\n\nparams <- list(booster = \"gbtree\", objective = \"binary:logistic\", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)\n\nxgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)\n\nmin(xgbcv$test.error.mean)\n\nxgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = \"error\")\n\nxgbpred <- predict (xgb1,dtest)\nxgbpred <- ifelse (xgbpred > 0.5,1,0)\n\nlibrary(caret)\nconfusionMatrix (xgbpred, ts_label)\n\nmat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)\nxgb.plot.importance (importance_matrix = mat[1:20]) \n\nlibrary(mlr)\n\nfact_col <- colnames(trainData)[sapply(trainData,is.character)]\n\nfor(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))\nfor(i in fact_col) set(testData,j=i,value = factor(testData[[i]]))\n\ntraintask <- makeClassifTask (data = trainData,target = \"target\")\ntesttask <- makeClassifTask (data = testData,target = \"target\")\n\ntraintask <- createDummyFeatures (obj = traintask) \ntesttask <- createDummyFeatures (obj = testtask)\n\nlrn <- makeLearner(\"classif.xgboost\",predict.type = \"response\")\nlrn$par.vals <- list( objective=\"binary:logistic\", eval_metric=\"error\", nrounds=100L, eta=0.1)\n\nparams <- makeParamSet( makeDiscreteParam(\"booster\",values = c(\"gbtree\",\"gblinear\")), makeIntegerParam(\"max_depth\",lower = 3L,upper = 10L), \n                        makeNumericParam(\"min_child_weight\",lower = 1L,upper = 10L), makeNumericParam(\"subsample\",lower = 0.5,upper = 1), \n                        makeNumericParam(\"colsample_bytree\",lower = 0.5,upper = 1))\n\nrdesc <- makeResampleDesc(\"CV\",stratify = T,iters=5L)\n\nctrl <- makeTuneControlRandom(maxit = 10L)\n\nlibrary(parallel)\nlibrary(parallelMap)\nparallelStartSocket(cpus = detectCores())\n\nmytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)\n\nlrn_tune <- setHyperPars(lrn,par.vals = mytune$x)\n\nxgmodel <- train(learner = lrn_tune,task = traintask)\n\nxgpred <- predict(xgmodel,testtask)\n\nconfusionMatrix(xgpred$data$response,xgpred$data$truth)\n  ",
    "created" : 1503669566940.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2908317562",
    "id" : "C852A754",
    "lastKnownWriteTime" : 1503681833,
    "last_content_update" : 1503681833825,
    "path" : "~/GitHub/RProjects/RandomForest/TestCode2.R",
    "project_path" : "TestCode2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}