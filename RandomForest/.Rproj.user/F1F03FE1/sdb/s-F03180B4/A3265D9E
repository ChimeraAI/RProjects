{
    "collab_server" : "",
    "contents" : "library(data.table)\n\n#Read and separate raw data\nrawData <- read.csv(\"bank-full.csv\", sep = \";\", header = TRUE)\n\n#Convert to DataFrame\nsetDT(rawData)\n\n#Set seed\nset.seed(11)\n\n#Shuffle Data\nrawData <- rawData[sample(nrow(rawData)),]\n\n#Split rawData into training and testing data\nlibrary(caTools)\ntempData <- sample.split(rawData$y, SplitRatio = 0.7)\ntrainData <- rawData[tempData]\ntestData <- rawData[!tempData]\n\ntr_target <- trainData$y\ntest_target <- testData$y\n\nnew_tr <- model.matrix(~.+0, data = trainData[,-c(\"y\")])\nnew_test <- model.matrix(~.+0, data = testData[,-c(\"y\")])\n\ntr_target <- as.numeric(tr_target) - 1\ntest_target <- as.numeric(test_target) - 1\n\nlibrary(xgboost)\ndTrain <- xgb.DMatrix(data = new_tr, label = tr_target)\ndTest <- xgb.DMatrix(data = new_test, label = test_target)\n\n\n# Setting parameters for xgboost\nparams <- list(booster = \"gbtree\", objective = \"binary:logistic\", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)\n\nxgbcv <- xgb.cv( params = params, data = dTrain, \n                 nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)\n\n#first default - model training\nxgb1 <- xgb.train (params = params, data = dTrain, nrounds = 79, \n                   watchlist = list(val=dTest,train=dTrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = \"error\")\n#model prediction\nxgbpred <- predict (xgb1,dTest)\nxgbpred <- ifelse (xgbpred > 0.5,1,0)\n\n#confusion matrix\nlibrary(caret)\nconfusionMatrix (xgbpred, test_target)\n\n#view variable importance plot\nmat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)\nxgb.plot.importance (importance_matrix = mat[1:20]) \n\nfact_col <- colnames(trainData)[sapply(trainData,is.character)]\n\nfor(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))\nfor (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))\n\nlibrary(mlr)\ntraintask <- makeClassifTask (data = trainData,target = \"y\")\ntesttask <- makeClassifTask (data = testData,target = \"y\")\n\ntraintask <- createDummyFeatures (obj = traintask)\ntesttask <- createDummyFeatures (obj = testtask)\n\nlrn <- makeLearner(\"classif.xgboost\",predict.type = \"response\")\nlrn$par.vals <- list( objective=\"binary:logistic\", eval_metric=\"error\", nrounds=100L, eta=0.1)\n\nparams <- makeParamSet( makeDiscreteParam(\"booster\",values = c(\"gbtree\",\"gblinear\")), \n                        makeIntegerParam(\"max_depth\",lower = 3L,upper = 10L), makeNumericParam(\"min_child_weight\",lower = 1L,upper = 10L), \n                        makeNumericParam(\"subsample\",lower = 0.5,upper = 1), makeNumericParam(\"colsample_bytree\",lower = 0.5,upper = 1))\n\nrdesc <- makeResampleDesc(\"CV\",stratify = T,iters=5L)\n\nctrl <- makeTuneControlRandom(maxit = 10L)\n\nlibrary(parallel)\nlibrary(parallelMap) \nparallelStartSocket(cpus = detectCores())\n\nmytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)\n\nlrn_tune <- setHyperPars(lrn,par.vals = mytune$x)\n\nxgmodel <- train(learner = lrn_tune,task = traintask)\n\nxgpred <- predict(xgmodel,testtask)\n\nconfusionMatrix(xgpred$data$response,xgpred$data$truth)\n\n\n\n\n\n",
    "created" : 1503072347726.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1340540837",
    "id" : "A3265D9E",
    "lastKnownWriteTime" : 1503931116,
    "last_content_update" : 1503941519111,
    "path" : "~/GitHub/RProjects/RandomForest/Code.R",
    "project_path" : "Code.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}