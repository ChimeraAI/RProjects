xgbcv <- xgb.cv( params = params, data = dTrain,
nrounds = 100, nfold = 10, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgb
#first default - model training
xgb1 <- xgb.train (params = params, data = dTrain, nrounds = 79,
watchlist = list(val=dTest,train=dTrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
#model prediction
xgbpred <- predict (xgb1,dTest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
#-----------------------------------------------------------------------------------
#view variable importance plot
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
#confusion matrix
confusionMatrix (xgbpred, test_target)
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
rawData$contact <- NULL
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
dTrain <- xgb.DMatrix(data = new_tr, label = tr_target)
dTest <- xgb.DMatrix(data = new_test, label = test_target)
#----------------------------------------------------------------------------------
# Setting parameters for xgboost
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.07, gamma=5, max_depth=7, min_child_weight=1, subsample=1, colsample_bytree=1, alpha = 1)
xgbcv <- xgb.cv( params = params, data = dTrain,
nrounds = 100, nfold = 10, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgb
#first default - model training
xgb1 <- xgb.train (params = params, data = dTrain, nrounds = 79,
watchlist = list(val=dTest,train=dTrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
#model prediction
xgbpred <- predict (xgb1,dTest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
#-----------------------------------------------------------------------------------
#view variable importance plot
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
#confusion matrix
confusionMatrix (xgbpred, test_target)
# Load Libraries
library(data.table)
library(caret)
library(xgboost)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
dTrain <- xgb.DMatrix(data = new_tr, label = tr_target)
dTest <- xgb.DMatrix(data = new_test, label = test_target)
#----------------------------------------------------------------------------------
# Setting parameters for xgboost
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.07, gamma=5, max_depth=7, min_child_weight=2, subsample=1, colsample_bytree=1, alpha = 1)
xgbcv <- xgb.cv( params = params, data = dTrain,
nrounds = 100, nfold = 10, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgb
#first default - model training
xgb1 <- xgb.train (params = params, data = dTrain, nrounds = 79,
watchlist = list(val=dTest,train=dTrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
#model prediction
xgbpred <- predict (xgb1,dTest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
#-----------------------------------------------------------------------------------
#view variable importance plot
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
#confusion matrix
confusionMatrix (xgbpred, test_target)
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.07, gamma=5, max_depth=7, min_child_weight=1, subsample=1, colsample_bytree=1, alpha = 2)
xgbcv <- xgb.cv( params = params, data = dTrain,
nrounds = 100, nfold = 10, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgb
#first default - model training
xgb1 <- xgb.train (params = params, data = dTrain, nrounds = 79,
watchlist = list(val=dTest,train=dTrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
#model prediction
xgbpred <- predict (xgb1,dTest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
#-----------------------------------------------------------------------------------
#view variable importance plot
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
#confusion matrix
confusionMatrix (xgbpred, test_target)
library(data.table)
library(caret)
library(xgboost)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
library(caTools)
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
dTrain <- xgb.DMatrix(data = new_tr, label = tr_target)
dTest <- xgb.DMatrix(data = new_test, label = test_target)
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
library(mlr)
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.1)
params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
ctrl <- makeTuneControlRandom(maxit = 10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
library(data.table)
library(caret)
library(xgboost)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
library(caTools)
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
dTrain <- xgb.DMatrix(data = new_tr, label = tr_target)
dTest <- xgb.DMatrix(data = new_test, label = test_target)
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
library(mlr)
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.07)
params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
ctrl <- makeTuneControlRandom(maxit = 10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
help(colnames)
fact_col
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
library(data.table)
library(caret)
library(xgboost)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
library(caTools)
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
library(mlr)
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.07)
params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
ctrl <- makeTuneControlRandom(maxit = 10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
library(data.table)
library(caret)
library(xgboost)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
library(caTools)
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
#
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
library(mlr)
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.07)
params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)
ctrl <- makeTuneControlRandom(maxit = 10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
help("makeLearner")
help(makeResampleDesc)
rm(list = ls())
# Use xgboost algorithm to make binary classification with grid search for hypertuning parameters
#Load libraries
library(data.table)
library(caret)
library(xgboost)
# Has the grid search option for hypertuning as oppose to xgboost library
library(mlr)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
tr_target <- trainData$y
test_target <- testData$y
new_tr <- model.matrix(~.+0, data = trainData[,-c("y")])
new_test <- model.matrix(~.+0, data = testData[,-c("y")])
tr_target <- as.numeric(tr_target) - 1
test_target <- as.numeric(test_target) - 1
#Convert characters to factors
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
#Create Tasks
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
# Perform one hot encoding
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
#Create Learner
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, colsample_bytree = 1,
booster = gbtree, subsample = 1, min_child_weight = 1, max_depth = 7)
params <- makeParamSet( makeNumericParam("eta",lower = 0.05L, upper = 0.15L),
makeIntegerParam("gamma",lower = 0, upper = 6),
makeNumericParam("alpha",lower = 0.5,upper = 2))
# 10-fold cross validation
rdesc <- makeResampleDesc("CV",stratify = T,iters=10L)
# Grid search
ctrl <- makeTuneControlGrid(resolution = 15)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
table(is.na(rawData))
sapply(rawData, function(x) sum(is.na(x))/length(x))
setDT(trainData)[,.N/nrow(trainData),target]
# Use xgboost algorithm to make binary classification with grid search for hypertuning parameters
#Load libraries
library(data.table)
library(caret)
library(xgboost)
# Has the grid search option for hypertuning as oppose to xgboost library
library(mlr)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Check for missing data
sapply(rawData, function(x) sum(is.na(x))/length(x))
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
setDT(trainData)[,.N/nrow(trainData),target]
setDT(trainData)[,.N/nrow(trainData),y]
setDT(testData)[,.N/nrow(testData),y]
# Use xgboost algorithm to make binary classification with grid search for hypertuning parameters
#Load libraries
library(data.table)
library(caret)
library(xgboost)
# Has the grid search option for hypertuning as oppose to xgboost library
library(mlr)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Check for missing data
sapply(rawData, function(x) sum(is.na(x))/length(x))
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
#Check for data imbalance between train and test data
setDT(trainData)[,.N/nrow(trainData),y]
setDT(testData)[,.N/nrow(testData),y]
tr_target <- trainData$y
test_target <- testData$y
#Convert characters to factors
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
#Create Tasks
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
#Create Learner
lrn <- makeLearner("classif.randomForest")
lrn$par.vals <- list( ntree = 100L, importance = TRUE)
rdesc <- makeResampleDesc("CV",iters=10L)
r <- resample(learner = rf.lrn, task = traintask, resampling = rdesc, measures = list(tpr,fpr,fnr,fpr,acc), show.info = T)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
ytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
xgmodel <- train(learner = lrn_tune,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
# Use xgboost algorithm to make binary classification with grid search for hypertuning parameters
#Load libraries
library(data.table)
library(caret)
library(xgboost)
# Has the grid search option for hypertuning as oppose to xgboost library
library(mlr)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Check for missing data
sapply(rawData, function(x) sum(is.na(x))/length(x))
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
#Check for data imbalance between train and test data
setDT(trainData)[,.N/nrow(trainData),y]
setDT(testData)[,.N/nrow(testData),y]
tr_target <- trainData$y
test_target <- testData$y
#Convert characters to factors
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
#Create Tasks
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
# Perform one hot encoding
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
#Create Learner
lrn <- makeLearner("classif.randomForest")
lrn$par.vals <- list( ntree = 100L, importance = TRUE)
rdesc <- makeResampleDesc("CV",iters=10L)
r <- resample(learner = rf.lrn, task = traintask, resampling = rdesc, measures = list(tpr,fpr,fnr,fpr,acc), show.info = T)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
xgmodel <- train(learner = lrn,task = traintask)
xgpred <- predict(xgmodel,testtask)
confusionMatrix(xgpred$data$response,xgpred$data$truth)
# Use xgboost algorithm to make binary classification with grid search for hypertuning parameters
#Load libraries
library(data.table)
library(caret)
library(xgboost)
# Has the grid search option for hypertuning as oppose to xgboost library
library(mlr)
library(caTools)
#Read and separate raw data
rawData <- read.csv("bank-full.csv", sep = ";", header = TRUE)
#Convert to DataFrame
setDT(rawData)
#Check for missing data
sapply(rawData, function(x) sum(is.na(x))/length(x))
#Set seed
set.seed(11)
#Split rawData into training and testing data
tempData <- sample.split(rawData$y, SplitRatio = 0.7)
trainData <- rawData[tempData]
testData <- rawData[!tempData]
#Check for data imbalance between train and test data
setDT(trainData)[,.N/nrow(trainData),y]
setDT(testData)[,.N/nrow(testData),y]
tr_target <- trainData$y
test_target <- testData$y
#Convert characters to factors
fact_col <- colnames(trainData)[sapply(trainData,is.character)]
for(i in fact_col) set(trainData,j=i,value = factor(trainData[[i]]))
for (i in fact_col) set(testData,j=i,value = factor(testData[[i]]))
# Perform one hot encoding
traintask <- createDummyFeatures (obj = traintask)
testtask <- createDummyFeatures (obj = testtask)
#Create Tasks
traintask <- makeClassifTask (data = trainData,target = "y")
testtask <- makeClassifTask (data = testData,target = "y")
#Create Learner
lrn <- makeLearner("classif.randomForest")
lrn$par.vals <- list( ntree = 100L, importance = TRUE)
rdesc <- makeResampleDesc("CV",iters=10L)
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = detectCores())
rf.lrn <- makeLearner("classif.randomForest")
rf.lrn$par.vals <- list(ntree = 100L,
importance=TRUE)
r <- resample(learner = rf.lrn
,task = traintask
,resampling = rdesc
,measures = list(tpr,fpr,fnr,fpr,acc)
,show.info = T)
source('~/GitHub/RProjects/BankingandMarketing/RandomForest.R')
install.packages(randomForest)
install.packages("randomForest")
source('~/GitHub/RProjects/BankingandMarketing/RandomForest.R')
source('~/GitHub/RProjects/BankingandMarketing/RandomForest.R')
source('~/GitHub/RProjects/BankingandMarketing/RandomForest.R')
source('~/GitHub/RProjects/BankingandMarketing/RandomForest.R')
rm(list = ls())
